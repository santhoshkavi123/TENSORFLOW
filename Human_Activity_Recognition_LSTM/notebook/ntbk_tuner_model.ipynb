{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "928a6e46",
   "metadata": {},
   "source": [
    "# Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d3465ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c03092db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kavisanthoshkumar/Library/CloudStorage/OneDrive-IllinoisInstituteofTechnology/Tensorflow/tensorflow_env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# basic packages\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#sklearn packages\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                             classification_report)\n",
    "\n",
    "# tensorflow packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Dense, \n",
    "                                     LSTM, \n",
    "                                     BatchNormalization,\n",
    "                                     Input)\n",
    "from tensorflow.keras.optimizers import (Adam, \n",
    "                                         AdamW)\n",
    "from tensorflow.keras.losses import (SparseCategoricalCrossentropy,\n",
    "                                     CategoricalCrossentropy)\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, \n",
    "                                        ModelCheckpoint)\n",
    "\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7870d6",
   "metadata": {},
   "source": [
    "# Load Train, Validation and Testing Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f745785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train : (5881, 128, 9) and y_train : (5881, 6)\n",
      "Shape of X_val : (1471, 128, 9) and y_train : (1471, 6)\n",
      "Shape of X_test : (2947, 128, 9) and y_train : (2947, 6)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"../data/processed/processed_array.npz\")\n",
    "\n",
    "X_train, y_train = data[\"x_train\"], data[\"y_train\"]\n",
    "X_val, y_val = data[\"x_val\"], data[\"y_val\"]\n",
    "X_test, y_test = data[\"x_test\"], data[\"y_test\"]\n",
    "\n",
    "print(f\"Shape of X_train : {X_train.shape} and y_train : {y_train.shape}\")\n",
    "print(f\"Shape of X_val : {X_val.shape} and y_train : {y_val.shape}\")\n",
    "print(f\"Shape of X_test : {X_test.shape} and y_train : {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf19814",
   "metadata": {},
   "source": [
    "# Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e0e8ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 12:21:20.452354: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-07-02 12:21:20.452379: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2025-07-02 12:21:20.452384: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751476880.452394  971586 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1751476880.452408  971586 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(64)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7aa03",
   "metadata": {},
   "source": [
    "# Keras Tuner Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d55e81",
   "metadata": {},
   "source": [
    "#### Step 1: Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e96618cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(Model):\n",
    "\n",
    "    def __init__(self, hp, num_classes):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        \n",
    "        # Tunable Parameters\n",
    "        lstm_units = hp.Int(\"units\", min_value = 32, max_value = 256, step = 32)\n",
    "    \n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm_layer = LSTM(units = lstm_units, \n",
    "                               activation = \"tanh\",\n",
    "                               recurrent_activation = 'sigmoid',\n",
    "                               )\n",
    "        \n",
    "        # Dense layers``\n",
    "        self.dense_layer_1 = Dense(units = 256, activation = \"leaky_relu\")\n",
    "        self.dense_layer_2 = Dense(units = 128, activation = \"leaky_relu\")\n",
    "        self.dense_layer_3 = Dense(units = 64, activation = \"leaky_relu\")\n",
    "\n",
    "        # Output layers\n",
    "        self.output_layer = Dense(units = num_classes, activation = \"softmax\")\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # lstm layer\n",
    "        x = self.lstm_layer(inputs)\n",
    "\n",
    "        # Dense layer\n",
    "        x = self.dense_layer_1(x)\n",
    "        x = self.dense_layer_2(x)\n",
    "        x = self.dense_layer_3(x)\n",
    "\n",
    "        # Output layer\n",
    "        out = self.output_layer(x)\n",
    "\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b59657",
   "metadata": {},
   "source": [
    "#### Step 2 : Keras Tuner Hyperband Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbdf8175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_tuner_hyperband_model(hp):\n",
    "\n",
    "    # Initiate model\n",
    "    model = CustomLSTM(hp, num_classes = 6)\n",
    "\n",
    "    # Compile the model\n",
    "    hp_optimizers = hp.Choice(\"optimizers\", values = [\"Adam\", \"AdamW\"]) \n",
    "    model.compile(optimizer = hp_optimizers, loss = CategoricalCrossentropy(), \n",
    "                  metrics = [\"accuracy\", \"f1_score\"])\n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00766a2",
   "metadata": {},
   "source": [
    "#### Step 3: Instantiate the tuner and perform hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d7d82ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(keras_tuner_hyperband_model, \n",
    "                     objective = 'val_accuracy',\n",
    "                     max_epochs = 3,\n",
    "                     directory = \"../model/\",\n",
    "                     project_name = \"har_tuner\"\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527b0fc8",
   "metadata": {},
   "source": [
    "#### Step 4: Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c3bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "        monitor = \"val_loss\", \n",
    "        verbose = 0,\n",
    "        mode = \"min\",\n",
    "        patience = 5\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5a54e",
   "metadata": {},
   "source": [
    "#### Step 5: Tuner Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58990728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first LSTM layer is 64 and the best choice of optimizer is\n",
      "AdamW\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner.search(train_dataset,  \n",
    "             epochs = 50, \n",
    "             validation_data = valid_dataset,\n",
    "             callbacks = callbacks)\n",
    "\n",
    "# Best Hyperparameters \n",
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first LSTM layer is {best_hps.get(\"units\")} and the best choice of optimizer is\n",
    "{best_hps.get(\"optimizers\")}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3d799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
